Using configurations:
{'BATCH_ITEM_NUM': 30,
 'BERT_LARGE': True,
 'BERT_LAYER': 11,
 'CONFIG_NAME': 'bertlarge_lstm_elissa',
 'CROSS_VALIDATION_FLAG': True,
 'CUDA': False,
 'ELMO_LAYER': 2,
 'ELMO_MODE': 'concat',
 'EVAL': {'BEST_EPOCH': 100, 'FLAG': False},
 'EXPERIMENT_NAME': 'bertlarge_lstm_elissa',
 'GLOVE_DIM': 100,
 'GPU_NUM': 0,
 'IS_BERT': True,
 'IS_ELMO': False,
 'IS_RANDOM': False,
 'KFOLDS': 5,
 'LSTM': {'ATTN': False,
          'BIDIRECTION': True,
          'DROP_PROB': 0.5,
          'FLAG': True,
          'HIDDEN_DIM': 200,
          'LAYERS': 2,
          'SEQ_LEN': 50},
 'MODE': 'train',
 'OUT_PATH': './',
 'PREDICTION_TYPE': 'rating',
 'PREDON': 'train',
 'RESUME_DIR': '',
 'SAVE_PREDS': False,
 'SEED': 0,
 'SINGLE_SENTENCE': True,
 'SOME_DATABASE': './sentence_si_means.csv',
 'SPLIT_NAME': '',
 'TRAIN': {'BATCH_SIZE': 32,
           'COEFF': {'BETA_1': 0.9, 'BETA_2': 0.999, 'EPS': 1e-08},
           'DROPOUT': {'FC_1': 0.75, 'FC_2': 0.75},
           'FLAG': True,
           'INTERVAL': 10,
           'LR': 0.0001,
           'LR_DECAY_EPOCH': 20,
           'LR_DECAY_RATE': 0.8,
           'START_EPOCH': 0,
           'TOTAL_EPOCH': 20}}
Using random seed 0.
Path to the current word embeddings: ./datasets/seed_0/bert_largelayer_11_lstm/embs_train_50.npy
Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt from cache at /Users/elissali/.cache/torch/pytorch_transformers/9b3c03a36e83b13d5ba95ac965c9f9074a99e14340c523ab405703179e79fc46.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json from cache at /Users/elissali/.cache/torch/pytorch_transformers/6dfaed860471b03ab5b9acb6153bea82b6632fb9bbe514d3fff050fe1319ee6d.fc076a4d5f1edf25ea3a2bd66e9f6f295dcd64c81dfef5b3f5a3eb2a82751ad1
Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin from cache at /Users/elissali/.cache/torch/pytorch_transformers/54da47087cc86ce75324e4dc9bbb5f66c6e83a7c6bd23baea8b489acc8d09aa4.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6
Start training
===============================
Fold #1
- - - - - - - - - - - - -
initializing neural net
[1/20][22/22] total train loss: 0.5491; total val loss: 0.1795 val r: 0.1258; time: 12.30sec
[2/20][22/22] total train loss: 0.5313; total val loss: 0.1890 val r: 0.2422; time: 9.08sec
[3/20][22/22] total train loss: 0.5137; total val loss: 0.1791 val r: 0.2727; time: 7.86sec
[4/20][22/22] total train loss: 0.4826; total val loss: 0.1805 val r: 0.3242; time: 7.84sec
[5/20][22/22] total train loss: 0.4458; total val loss: 0.1936 val r: 0.3049; time: 7.70sec
[6/20][22/22] total train loss: 0.4197; total val loss: 0.1943 val r: 0.3186; time: 8.16sec
[7/20][22/22] total train loss: 0.3577; total val loss: 0.2076 val r: 0.3357; time: 8.71sec
[8/20][22/22] total train loss: 0.3187; total val loss: 0.2290 val r: 0.3509; time: 8.10sec
[9/20][22/22] total train loss: 0.2531; total val loss: 0.2167 val r: 0.4226; time: 8.75sec
[10/20][22/22] total train loss: 0.2014; total val loss: 0.2159 val r: 0.4133; time: 9.03sec
[11/20][22/22] total train loss: 0.1531; total val loss: 0.2596 val r: 0.3620; time: 9.40sec
[12/20][22/22] total train loss: 0.1242; total val loss: 0.2281 val r: 0.3991; time: 9.15sec
[13/20][22/22] total train loss: 0.1005; total val loss: 0.2321 val r: 0.3352; time: 8.51sec
[14/20][22/22] total train loss: 0.0751; total val loss: 0.2309 val r: 0.3056; time: 8.91sec
[15/20][22/22] total train loss: 0.0580; total val loss: 0.2398 val r: 0.3319; time: 8.82sec
[16/20][22/22] total train loss: 0.0485; total val loss: 0.2413 val r: 0.3372; time: 9.26sec
[17/20][22/22] total train loss: 0.0476; total val loss: 0.2375 val r: 0.2998; time: 8.12sec
[18/20][22/22] total train loss: 0.0407; total val loss: 0.2443 val r: 0.3291; time: 8.57sec
[19/20][22/22] total train loss: 0.0374; total val loss: 0.2295 val r: 0.3144; time: 8.39sec
learning rate updated: 8e-05
[20/20][22/22] total train loss: 0.0337; total val loss: 0.2258 val r: 0.3166; time: 8.27sec
Best epoch 9 with val_r = 0.4226.
Fold #2
- - - - - - - - - - - - -
initializing neural net
[1/20][22/22] total train loss: 0.5288; total val loss: 0.2093 val r: 0.1152; time: 12.03sec
[2/20][22/22] total train loss: 0.5066; total val loss: 0.2064 val r: 0.2559; time: 8.51sec
[3/20][22/22] total train loss: 0.4879; total val loss: 0.1982 val r: 0.2605; time: 9.72sec
[4/20][22/22] total train loss: 0.4595; total val loss: 0.2161 val r: 0.2632; time: 13.75sec
[5/20][22/22] total train loss: 0.4129; total val loss: 0.2145 val r: 0.3032; time: 8.94sec
[6/20][22/22] total train loss: 0.3897; total val loss: 0.2455 val r: 0.3064; time: 8.77sec
[7/20][22/22] total train loss: 0.3350; total val loss: 0.2360 val r: 0.3356; time: 8.27sec
[8/20][22/22] total train loss: 0.2566; total val loss: 0.2515 val r: 0.3094; time: 8.73sec
[9/20][22/22] total train loss: 0.2105; total val loss: 0.2656 val r: 0.2744; time: 8.17sec
[10/20][22/22] total train loss: 0.1583; total val loss: 0.2729 val r: 0.2421; time: 8.64sec
[11/20][22/22] total train loss: 0.1112; total val loss: 0.2907 val r: 0.2596; time: 10.18sec
[12/20][22/22] total train loss: 0.0958; total val loss: 0.2680 val r: 0.2208; time: 8.62sec
[13/20][22/22] total train loss: 0.0786; total val loss: 0.2862 val r: 0.2208; time: 8.74sec
[14/20][22/22] total train loss: 0.0551; total val loss: 0.2791 val r: 0.1938; time: 8.74sec
[15/20][22/22] total train loss: 0.0430; total val loss: 0.2822 val r: 0.2057; time: 8.52sec
[16/20][22/22] total train loss: 0.0347; total val loss: 0.2747 val r: 0.2062; time: 9.29sec
[17/20][22/22] total train loss: 0.0333; total val loss: 0.2754 val r: 0.2090; time: 8.56sec
[18/20][22/22] total train loss: 0.0278; total val loss: 0.2831 val r: 0.2172; time: 8.83sec
[19/20][22/22] total train loss: 0.0249; total val loss: 0.2980 val r: 0.2089; time: 9.37sec
learning rate updated: 8e-05
[20/20][22/22] total train loss: 0.0230; total val loss: 0.2826 val r: 0.2235; time: 10.36sec
Best epoch 7 with val_r = 0.3356.
Fold #3
- - - - - - - - - - - - -
initializing neural net
[1/20][22/22] total train loss: 0.5990; total val loss: 0.1224 val r: 0.1185; time: 21.56sec
[2/20][22/22] total train loss: 0.5760; total val loss: 0.1233 val r: 0.2122; time: 9.64sec
[3/20][22/22] total train loss: 0.5556; total val loss: 0.1268 val r: 0.2488; time: 8.31sec
[4/20][22/22] total train loss: 0.5069; total val loss: 0.1281 val r: 0.2648; time: 8.43sec
[5/20][22/22] total train loss: 0.4532; total val loss: 0.1556 val r: 0.3265; time: 8.20sec
[6/20][22/22] total train loss: 0.3946; total val loss: 0.1567 val r: 0.3568; time: 8.42sec
[7/20][22/22] total train loss: 0.3355; total val loss: 0.1733 val r: 0.3612; time: 8.39sec
[8/20][22/22] total train loss: 0.2741; total val loss: 0.1925 val r: 0.3478; time: 10.15sec
[9/20][22/22] total train loss: 0.2282; total val loss: 0.1865 val r: 0.3357; time: 7.64sec
[10/20][22/22] total train loss: 0.1660; total val loss: 0.2099 val r: 0.2980; time: 7.66sec
[11/20][22/22] total train loss: 0.1258; total val loss: 0.2125 val r: 0.3063; time: 8.20sec
[12/20][22/22] total train loss: 0.1028; total val loss: 0.2074 val r: 0.2720; time: 9.04sec
[13/20][22/22] total train loss: 0.0941; total val loss: 0.2270 val r: 0.2976; time: 9.48sec
[14/20][22/22] total train loss: 0.0744; total val loss: 0.2172 val r: 0.2843; time: 8.00sec
[15/20][22/22] total train loss: 0.0567; total val loss: 0.2113 val r: 0.3015; time: 8.32sec
[16/20][22/22] total train loss: 0.0494; total val loss: 0.1975 val r: 0.2879; time: 8.60sec
[17/20][22/22] total train loss: 0.0423; total val loss: 0.2059 val r: 0.2749; time: 10.74sec
[18/20][22/22] total train loss: 0.0359; total val loss: 0.2088 val r: 0.2936; time: 17.83sec
[19/20][22/22] total train loss: 0.0325; total val loss: 0.2036 val r: 0.2971; time: 12.40sec
learning rate updated: 8e-05
[20/20][22/22] total train loss: 0.0311; total val loss: 0.2111 val r: 0.3011; time: 10.31sec
Best epoch 7 with val_r = 0.3612.
Fold #4
- - - - - - - - - - - - -
initializing neural net
[1/20][22/22] total train loss: 0.5958; total val loss: 0.1378 val r: 0.0697; time: 12.84sec
[2/20][22/22] total train loss: 0.5717; total val loss: 0.1391 val r: 0.2392; time: 9.14sec
[3/20][22/22] total train loss: 0.5505; total val loss: 0.1282 val r: 0.2438; time: 8.21sec
[4/20][22/22] total train loss: 0.5099; total val loss: 0.1392 val r: 0.2515; time: 10.07sec
[5/20][22/22] total train loss: 0.4590; total val loss: 0.1641 val r: 0.3023; time: 12.59sec
[6/20][22/22] total train loss: 0.4122; total val loss: 0.1553 val r: 0.3322; time: 8.42sec
[7/20][22/22] total train loss: 0.3540; total val loss: 0.1637 val r: 0.3613; time: 7.78sec
[8/20][22/22] total train loss: 0.2888; total val loss: 0.1895 val r: 0.3967; time: 7.79sec
[9/20][22/22] total train loss: 0.2266; total val loss: 0.2141 val r: 0.3677; time: 7.86sec
[10/20][22/22] total train loss: 0.1800; total val loss: 0.1992 val r: 0.3549; time: 8.30sec
[11/20][22/22] total train loss: 0.1314; total val loss: 0.2082 val r: 0.3603; time: 8.46sec
[12/20][22/22] total train loss: 0.1014; total val loss: 0.2046 val r: 0.3511; time: 9.03sec
[13/20][22/22] total train loss: 0.0761; total val loss: 0.2296 val r: 0.3575; time: 9.02sec
[14/20][22/22] total train loss: 0.0612; total val loss: 0.2315 val r: 0.3146; time: 8.11sec
[15/20][22/22] total train loss: 0.0568; total val loss: 0.2011 val r: 0.2842; time: 8.01sec
[16/20][22/22] total train loss: 0.0535; total val loss: 0.2114 val r: 0.3366; time: 7.90sec
[17/20][22/22] total train loss: 0.0416; total val loss: 0.2216 val r: 0.3131; time: 8.88sec
[18/20][22/22] total train loss: 0.0398; total val loss: 0.2094 val r: 0.3162; time: 7.86sec
[19/20][22/22] total train loss: 0.0309; total val loss: 0.2237 val r: 0.3203; time: 7.84sec
learning rate updated: 8e-05
[20/20][22/22] total train loss: 0.0295; total val loss: 0.2028 val r: 0.3146; time: 7.76sec
Best epoch 8 with val_r = 0.3967.
Fold #5
- - - - - - - - - - - - -
initializing neural net
[1/20][22/22] total train loss: 0.5786; total val loss: 0.1553 val r: 0.1713; time: 7.76sec
[2/20][22/22] total train loss: 0.5585; total val loss: 0.1441 val r: 0.2452; time: 9.43sec
[3/20][22/22] total train loss: 0.5404; total val loss: 0.1481 val r: 0.3631; time: 7.92sec
[4/20][22/22] total train loss: 0.5087; total val loss: 0.1585 val r: 0.3378; time: 7.79sec
[5/20][22/22] total train loss: 0.4556; total val loss: 0.1794 val r: 0.3530; time: 9.30sec
[6/20][22/22] total train loss: 0.4001; total val loss: 0.1891 val r: 0.3195; time: 10.10sec
[7/20][22/22] total train loss: 0.3311; total val loss: 0.2047 val r: 0.2753; time: 8.45sec
[8/20][22/22] total train loss: 0.2662; total val loss: 0.2140 val r: 0.2536; time: 8.43sec
[9/20][22/22] total train loss: 0.2088; total val loss: 0.2274 val r: 0.2561; time: 8.58sec
[10/20][22/22] total train loss: 0.1610; total val loss: 0.2336 val r: 0.2593; time: 8.14sec
[11/20][22/22] total train loss: 0.1198; total val loss: 0.2414 val r: 0.2349; time: 8.40sec
[12/20][22/22] total train loss: 0.0921; total val loss: 0.2397 val r: 0.2403; time: 9.36sec
[13/20][22/22] total train loss: 0.0727; total val loss: 0.2544 val r: 0.2293; time: 8.85sec
[14/20][22/22] total train loss: 0.0591; total val loss: 0.2549 val r: 0.2543; time: 10.13sec
[15/20][22/22] total train loss: 0.0468; total val loss: 0.2377 val r: 0.2346; time: 7.84sec
[16/20][22/22] total train loss: 0.0399; total val loss: 0.2464 val r: 0.2228; time: 7.85sec
[17/20][22/22] total train loss: 0.0431; total val loss: 0.2590 val r: 0.2370; time: 7.71sec
[18/20][22/22] total train loss: 0.0363; total val loss: 0.2566 val r: 0.2501; time: 7.74sec
[19/20][22/22] total train loss: 0.0285; total val loss: 0.2507 val r: 0.2224; time: 8.07sec
learning rate updated: 8e-05
[20/20][22/22] total train loss: 0.0251; total val loss: 0.2371 val r: 0.2082; time: 8.25sec
Best epoch 3 with val_r = 0.3631.
Highest avg. r=0.3338 achieved at epoch 7 (on validation set).
Avg. train loss: [0.5702581983059645, 0.5488203641027212, 0.5296416971832514, 0.493528201431036, 0.4453015372157097, 0.4032580818980932, 0.34265819415450094, 0.28088238565251233, 0.22544244173914194, 0.17336179865524173, 0.1282771152909845, 0.10323834558948874, 0.08440492190420627, 0.06498233403544873, 0.05223451127531007, 0.045224720204714686, 0.04159607534529641, 0.036127029091585425, 0.030840843613259495, 0.028460919030476362]
Avg. validation loss: [0.1608482774347067, 0.16036972105503083, 0.15607298463582991, 0.16448941603302955, 0.18143802955746652, 0.18817715644836425, 0.197053225338459, 0.21529368609189986, 0.22206859849393368, 0.22629749290645124, 0.24247466064989567, 0.2295621182769537, 0.24587371982634068, 0.24270918183028697, 0.2344223778694868, 0.23424566388130189, 0.23987522087991237, 0.24043258503079415, 0.24108811393380164, 0.23187591433525084]
Avg. validation r: [0.12011511377281692, 0.23894522576905924, 0.27777972015524544, 0.28829917944300393, 0.3179768617832752, 0.3267105405175913, 0.3338104385165539, 0.3316695624392052, 0.33130700383112277, 0.3135326279160138, 0.3046284027530993, 0.29665689350524715, 0.2880557645270284, 0.2705100679911346, 0.2715870278384819, 0.27812303004100025, 0.266743705248253, 0.2812265410595601, 0.272614881109764, 0.2728283395325335]
Using configurations:
{'BATCH_ITEM_NUM': 30,
 'BERT_LARGE': True,
 'BERT_LAYER': 11,
 'CONFIG_NAME': 'bertlarge_lstm_elissa',
 'CROSS_VALIDATION_FLAG': True,
 'CUDA': False,
 'ELMO_LAYER': 2,
 'ELMO_MODE': 'concat',
 'EVAL': {'BEST_EPOCH': 100, 'FLAG': False},
 'EXPERIMENT_NAME': 'bertlarge_lstm_elissa',
 'GLOVE_DIM': 100,
 'GPU_NUM': 0,
 'IS_BERT': True,
 'IS_ELMO': False,
 'IS_RANDOM': False,
 'KFOLDS': 5,
 'LSTM': {'ATTN': False,
          'BIDIRECTION': True,
          'DROP_PROB': 0.5,
          'FLAG': True,
          'HIDDEN_DIM': 200,
          'LAYERS': 2,
          'SEQ_LEN': 50},
 'MODE': 'train',
 'OUT_PATH': './',
 'PREDICTION_TYPE': 'rating',
 'PREDON': 'train',
 'RESUME_DIR': '',
 'SAVE_PREDS': False,
 'SEED': 0,
 'SINGLE_SENTENCE': True,
 'SOME_DATABASE': './sentence_si_means.csv',
 'SPLIT_NAME': '',
 'TRAIN': {'BATCH_SIZE': 32,
           'COEFF': {'BETA_1': 0.9, 'BETA_2': 0.999, 'EPS': 1e-08},
           'DROPOUT': {'FC_1': 0.75, 'FC_2': 0.75},
           'FLAG': True,
           'INTERVAL': 10,
           'LR': 0.001,
           'LR_DECAY_EPOCH': 20,
           'LR_DECAY_RATE': 0.8,
           'START_EPOCH': 0,
           'TOTAL_EPOCH': 20}}
Using random seed 0.
Path to the current word embeddings: ./datasets/seed_0/bert_largelayer_11_lstm/embs_train_50.npy
Start training
===============================
Fold #1
- - - - - - - - - - - - -
initializing neural net
[1/20][22/22] total train loss: 0.5427; total val loss: 0.1720 val r: 0.3279; time: 9.67sec
[2/20][22/22] total train loss: 0.4355; total val loss: 0.1913 val r: 0.3545; time: 7.92sec
[3/20][22/22] total train loss: 0.3140; total val loss: 0.1859 val r: 0.2982; time: 7.35sec
[4/20][22/22] total train loss: 0.1882; total val loss: 0.2496 val r: 0.3010; time: 9.16sec
[5/20][22/22] total train loss: 0.0998; total val loss: 0.2302 val r: 0.2918; time: 10.24sec
[6/20][22/22] total train loss: 0.0677; total val loss: 0.2204 val r: 0.3185; time: 8.95sec
[7/20][22/22] total train loss: 0.0505; total val loss: 0.2239 val r: 0.3311; time: 9.37sec
[8/20][22/22] total train loss: 0.0456; total val loss: 0.2211 val r: 0.3471; time: 9.22sec
[9/20][22/22] total train loss: 0.0337; total val loss: 0.2103 val r: 0.3241; time: 9.08sec
[10/20][22/22] total train loss: 0.0327; total val loss: 0.2269 val r: 0.3720; time: 9.60sec
[11/20][22/22] total train loss: 0.0248; total val loss: 0.2284 val r: 0.3419; time: 8.20sec
[12/20][22/22] total train loss: 0.0194; total val loss: 0.2272 val r: 0.3300; time: 8.53sec
[13/20][22/22] total train loss: 0.0158; total val loss: 0.2143 val r: 0.3193; time: 7.94sec
[14/20][22/22] total train loss: 0.0137; total val loss: 0.2201 val r: 0.3404; time: 8.64sec
[15/20][22/22] total train loss: 0.0123; total val loss: 0.2058 val r: 0.3223; time: 7.54sec
[16/20][22/22] total train loss: 0.0105; total val loss: 0.2205 val r: 0.3450; time: 8.43sec
[17/20][22/22] total train loss: 0.0094; total val loss: 0.2154 val r: 0.3275; time: 8.50sec
[18/20][22/22] total train loss: 0.0095; total val loss: 0.2125 val r: 0.3354; time: 11.71sec
[19/20][22/22] total train loss: 0.0101; total val loss: 0.2169 val r: 0.3368; time: 13.85sec
learning rate updated: 0.0008
[20/20][22/22] total train loss: 0.0087; total val loss: 0.2093 val r: 0.3340; time: 13.35sec
Best epoch 10 with val_r = 0.3720.
Fold #2
- - - - - - - - - - - - -
initializing neural net
[1/20][22/22] total train loss: 0.5183; total val loss: 0.2076 val r: 0.2620; time: 12.71sec
[2/20][22/22] total train loss: 0.4334; total val loss: 0.2030 val r: 0.3022; time: 11.63sec
[3/20][22/22] total train loss: 0.3013; total val loss: 0.2187 val r: 0.3247; time: 11.22sec
[4/20][22/22] total train loss: 0.2200; total val loss: 0.2475 val r: 0.2826; time: 10.46sec
[5/20][22/22] total train loss: 0.1162; total val loss: 0.2722 val r: 0.2706; time: 8.90sec
[6/20][22/22] total train loss: 0.0678; total val loss: 0.2690 val r: 0.3189; time: 8.17sec
[7/20][22/22] total train loss: 0.0559; total val loss: 0.2580 val r: 0.3389; time: 8.08sec
[8/20][22/22] total train loss: 0.0379; total val loss: 0.2545 val r: 0.3266; time: 8.21sec
[9/20][22/22] total train loss: 0.0254; total val loss: 0.2668 val r: 0.2922; time: 8.07sec
[10/20][22/22] total train loss: 0.0235; total val loss: 0.2568 val r: 0.3161; time: 8.06sec
[11/20][22/22] total train loss: 0.0187; total val loss: 0.2442 val r: 0.3032; time: 8.20sec
[12/20][22/22] total train loss: 0.0195; total val loss: 0.2733 val r: 0.3054; time: 8.44sec
[13/20][22/22] total train loss: 0.0150; total val loss: 0.2737 val r: 0.3056; time: 9.11sec
[14/20][22/22] total train loss: 0.0140; total val loss: 0.2459 val r: 0.3191; time: 8.27sec
[15/20][22/22] total train loss: 0.0111; total val loss: 0.2507 val r: 0.3303; time: 8.19sec
[16/20][22/22] total train loss: 0.0106; total val loss: 0.2504 val r: 0.3004; time: 8.27sec
[17/20][22/22] total train loss: 0.0117; total val loss: 0.2556 val r: 0.3201; time: 8.19sec
[18/20][22/22] total train loss: 0.0106; total val loss: 0.2696 val r: 0.3043; time: 8.04sec
[19/20][22/22] total train loss: 0.0089; total val loss: 0.2489 val r: 0.2974; time: 8.20sec
learning rate updated: 0.0008
[20/20][22/22] total train loss: 0.0095; total val loss: 0.2405 val r: 0.3156; time: 8.19sec
Best epoch 7 with val_r = 0.3389.
Fold #3
- - - - - - - - - - - - -
initializing neural net
[1/20][22/22] total train loss: 0.5926; total val loss: 0.1386 val r: 0.2783; time: 8.85sec
[2/20][22/22] total train loss: 0.4796; total val loss: 0.1401 val r: 0.2799; time: 8.53sec
[3/20][22/22] total train loss: 0.3151; total val loss: 0.1814 val r: 0.2484; time: 7.94sec
[4/20][22/22] total train loss: 0.1893; total val loss: 0.1969 val r: 0.2115; time: 7.99sec
[5/20][22/22] total train loss: 0.1610; total val loss: 0.1897 val r: 0.2609; time: 8.81sec
[6/20][22/22] total train loss: 0.0993; total val loss: 0.2000 val r: 0.2410; time: 9.97sec
[7/20][22/22] total train loss: 0.0558; total val loss: 0.1823 val r: 0.2543; time: 9.58sec
[8/20][22/22] total train loss: 0.0363; total val loss: 0.1705 val r: 0.2867; time: 9.85sec
[9/20][22/22] total train loss: 0.0288; total val loss: 0.1952 val r: 0.2353; time: 8.31sec
[10/20][22/22] total train loss: 0.0219; total val loss: 0.1918 val r: 0.2744; time: 8.24sec
[11/20][22/22] total train loss: 0.0168; total val loss: 0.1838 val r: 0.2801; time: 9.87sec
[12/20][22/22] total train loss: 0.0141; total val loss: 0.1932 val r: 0.2657; time: 9.58sec
[13/20][22/22] total train loss: 0.0125; total val loss: 0.1788 val r: 0.2786; time: 8.64sec
[14/20][22/22] total train loss: 0.0122; total val loss: 0.1869 val r: 0.2763; time: 7.94sec
[15/20][22/22] total train loss: 0.0127; total val loss: 0.1817 val r: 0.2524; time: 11.35sec
[16/20][22/22] total train loss: 0.0120; total val loss: 0.1851 val r: 0.2920; time: 10.79sec
[17/20][22/22] total train loss: 0.0114; total val loss: 0.1812 val r: 0.2709; time: 8.08sec
[18/20][22/22] total train loss: 0.0108; total val loss: 0.1809 val r: 0.2860; time: 8.95sec
[19/20][22/22] total train loss: 0.0108; total val loss: 0.1807 val r: 0.2793; time: 8.61sec
learning rate updated: 0.0008
[20/20][22/22] total train loss: 0.0096; total val loss: 0.1924 val r: 0.2749; time: 8.52sec
Best epoch 16 with val_r = 0.2920.
Fold #4
- - - - - - - - - - - - -
initializing neural net
[1/20][22/22] total train loss: 0.5983; total val loss: 0.1315 val r: 0.2094; time: 9.44sec
[2/20][22/22] total train loss: 0.4912; total val loss: 0.1469 val r: 0.2517; time: 10.31sec
[3/20][22/22] total train loss: 0.3800; total val loss: 0.1606 val r: 0.2857; time: 8.37sec
[4/20][22/22] total train loss: 0.2604; total val loss: 0.2081 val r: 0.3747; time: 8.09sec
[5/20][22/22] total train loss: 0.1459; total val loss: 0.1939 val r: 0.3114; time: 8.05sec
[6/20][22/22] total train loss: 0.1165; total val loss: 0.1927 val r: 0.3327; time: 8.07sec
[7/20][22/22] total train loss: 0.0642; total val loss: 0.1820 val r: 0.2770; time: 8.39sec
[8/20][22/22] total train loss: 0.0384; total val loss: 0.1871 val r: 0.3307; time: 8.09sec
[9/20][22/22] total train loss: 0.0285; total val loss: 0.1793 val r: 0.3077; time: 8.20sec
[10/20][22/22] total train loss: 0.0212; total val loss: 0.1888 val r: 0.2981; time: 8.32sec
[11/20][22/22] total train loss: 0.0179; total val loss: 0.1784 val r: 0.2846; time: 8.42sec
[12/20][22/22] total train loss: 0.0158; total val loss: 0.1895 val r: 0.3097; time: 8.24sec
[13/20][22/22] total train loss: 0.0151; total val loss: 0.1932 val r: 0.2920; time: 8.25sec
[14/20][22/22] total train loss: 0.0133; total val loss: 0.2008 val r: 0.3254; time: 8.06sec
[15/20][22/22] total train loss: 0.0132; total val loss: 0.1870 val r: 0.2893; time: 8.04sec
[16/20][22/22] total train loss: 0.0128; total val loss: 0.1834 val r: 0.3072; time: 8.15sec
[17/20][22/22] total train loss: 0.0135; total val loss: 0.1929 val r: 0.2975; time: 8.15sec
[18/20][22/22] total train loss: 0.0121; total val loss: 0.1831 val r: 0.3101; time: 8.08sec
[19/20][22/22] total train loss: 0.0105; total val loss: 0.1925 val r: 0.3069; time: 8.12sec
learning rate updated: 0.0008
[20/20][22/22] total train loss: 0.0101; total val loss: 0.1793 val r: 0.3056; time: 8.01sec
Best epoch 4 with val_r = 0.3747.
Fold #5
- - - - - - - - - - - - -
initializing neural net
[1/20][22/22] total train loss: 0.5715; total val loss: 0.1596 val r: 0.3444; time: 8.64sec
[2/20][22/22] total train loss: 0.4661; total val loss: 0.1575 val r: 0.3000; time: 8.93sec
[3/20][22/22] total train loss: 0.3352; total val loss: 0.2029 val r: 0.3125; time: 8.25sec
[4/20][22/22] total train loss: 0.2000; total val loss: 0.2128 val r: 0.2839; time: 8.01sec
[5/20][22/22] total train loss: 0.1374; total val loss: 0.2304 val r: 0.2642; time: 8.14sec
[6/20][22/22] total train loss: 0.0774; total val loss: 0.2362 val r: 0.2878; time: 8.34sec
[7/20][22/22] total train loss: 0.0451; total val loss: 0.2223 val r: 0.2726; time: 8.21sec
[8/20][22/22] total train loss: 0.0310; total val loss: 0.2214 val r: 0.2965; time: 8.17sec
[9/20][22/22] total train loss: 0.0285; total val loss: 0.2289 val r: 0.2416; time: 8.12sec
[10/20][22/22] total train loss: 0.0245; total val loss: 0.2143 val r: 0.2871; time: 8.05sec
[11/20][22/22] total train loss: 0.0206; total val loss: 0.2216 val r: 0.2414; time: 8.33sec
[12/20][22/22] total train loss: 0.0162; total val loss: 0.2158 val r: 0.2620; time: 8.36sec
[13/20][22/22] total train loss: 0.0129; total val loss: 0.2150 val r: 0.2649; time: 8.13sec
[14/20][22/22] total train loss: 0.0125; total val loss: 0.2175 val r: 0.2610; time: 8.04sec
[15/20][22/22] total train loss: 0.0130; total val loss: 0.2090 val r: 0.2794; time: 8.58sec
[16/20][22/22] total train loss: 0.0113; total val loss: 0.2206 val r: 0.2844; time: 8.18sec
[17/20][22/22] total train loss: 0.0109; total val loss: 0.2114 val r: 0.2634; time: 8.10sec
[18/20][22/22] total train loss: 0.0102; total val loss: 0.2194 val r: 0.2600; time: 8.15sec
[19/20][22/22] total train loss: 0.0097; total val loss: 0.2049 val r: 0.2715; time: 8.25sec
learning rate updated: 0.0008
[20/20][22/22] total train loss: 0.0088; total val loss: 0.2190 val r: 0.2704; time: 8.30sec
Best epoch 1 with val_r = 0.3444.
Highest avg. r=0.3175 achieved at epoch 8 (on validation set).
Avg. train loss: [0.5646700501441956, 0.46115062218159436, 0.32911822190508244, 0.21159574063494802, 0.13204995314590634, 0.0857455131597817, 0.05429933844134212, 0.03785560212563723, 0.0289981798036024, 0.024760743393562733, 0.019743922480847685, 0.01700517943245359, 0.014280761079862713, 0.013124723365763202, 0.012456629070220515, 0.011457754787988961, 0.011386370440595784, 0.01062714665895328, 0.009982038699672558, 0.00935661032272037]
Avg. validation loss: [0.16184258684515954, 0.16774160377681255, 0.1899037979543209, 0.22295711617916822, 0.22327620722353458, 0.22367047071456908, 0.21371592432260514, 0.21089841574430465, 0.21608022190630435, 0.21573286466300487, 0.21130611449480058, 0.21979306899011136, 0.21501358170062304, 0.21424826830625535, 0.20681496970355512, 0.21199266985058784, 0.21131937243044377, 0.21310877054929733, 0.20877189487218856, 0.2080897405743599]
Avg. validation r: [0.28436902948167575, 0.2976696262879602, 0.29390247428965177, 0.2907524095251451, 0.27977024988386334, 0.2997873077458142, 0.2947976154555071, 0.31751773701779523, 0.28017437268669243, 0.30956625379144637, 0.2902192636724998, 0.294580914428367, 0.29209379945949376, 0.30442648007479484, 0.29472645349829885, 0.3057890248814647, 0.295860792911045, 0.29917814411234167, 0.2983719883174662, 0.3001042981536632]
Using configurations:
{'BATCH_ITEM_NUM': 30,
 'BERT_LARGE': True,
 'BERT_LAYER': 11,
 'CONFIG_NAME': 'bertlarge_lstm_elissa',
 'CROSS_VALIDATION_FLAG': True,
 'CUDA': False,
 'ELMO_LAYER': 2,
 'ELMO_MODE': 'concat',
 'EVAL': {'BEST_EPOCH': 100, 'FLAG': False},
 'EXPERIMENT_NAME': 'bertlarge_lstm_elissa',
 'GLOVE_DIM': 100,
 'GPU_NUM': 0,
 'IS_BERT': True,
 'IS_ELMO': False,
 'IS_RANDOM': False,
 'KFOLDS': 5,
 'LSTM': {'ATTN': False,
          'BIDIRECTION': True,
          'DROP_PROB': 0.5,
          'FLAG': True,
          'HIDDEN_DIM': 800,
          'LAYERS': 2,
          'SEQ_LEN': 50},
 'MODE': 'train',
 'OUT_PATH': './',
 'PREDICTION_TYPE': 'rating',
 'PREDON': 'train',
 'RESUME_DIR': '',
 'SAVE_PREDS': False,
 'SEED': 0,
 'SINGLE_SENTENCE': True,
 'SOME_DATABASE': './sentence_si_means.csv',
 'SPLIT_NAME': '',
 'TRAIN': {'BATCH_SIZE': 32,
           'COEFF': {'BETA_1': 0.9, 'BETA_2': 0.999, 'EPS': 1e-08},
           'DROPOUT': {'FC_1': 0.75, 'FC_2': 0.75},
           'FLAG': True,
           'INTERVAL': 10,
           'LR': 0.001,
           'LR_DECAY_EPOCH': 20,
           'LR_DECAY_RATE': 0.8,
           'START_EPOCH': 0,
           'TOTAL_EPOCH': 20}}
Using random seed 0.
Path to the current word embeddings: ./datasets/seed_0/bert_largelayer_11_lstm/embs_train_50.npy
Start training
===============================
Fold #1
- - - - - - - - - - - - -
initializing neural net
[1/20][22/22] total train loss: 0.8560; total val loss: 0.1791 val r: 0.0753; time: 79.93sec
[2/20][22/22] total train loss: 0.4774; total val loss: 0.1909 val r: 0.2136; time: 80.62sec
[3/20][22/22] total train loss: 0.3586; total val loss: 0.1882 val r: 0.2343; time: 91.50sec
[4/20][22/22] total train loss: 0.2721; total val loss: 0.2407 val r: 0.2608; time: 89.51sec
[5/20][22/22] total train loss: 0.2095; total val loss: 0.2245 val r: 0.3099; time: 78.41sec
[6/20][22/22] total train loss: 0.1171; total val loss: 0.2353 val r: 0.3256; time: 77.62sec
[7/20][22/22] total train loss: 0.0822; total val loss: 0.2220 val r: 0.2728; time: 80.24sec
[8/20][22/22] total train loss: 0.0556; total val loss: 0.2230 val r: 0.2816; time: 77.03sec
[9/20][22/22] total train loss: 0.0502; total val loss: 0.2212 val r: 0.3210; time: 77.71sec
[10/20][22/22] total train loss: 0.0397; total val loss: 0.2240 val r: 0.2882; time: 77.33sec
[11/20][22/22] total train loss: 0.0320; total val loss: 0.2106 val r: 0.2707; time: 79.68sec
[12/20][22/22] total train loss: 0.0274; total val loss: 0.2291 val r: 0.3173; time: 80.20sec
[13/20][22/22] total train loss: 0.0259; total val loss: 0.1972 val r: 0.2968; time: 79.04sec
[14/20][22/22] total train loss: 0.0248; total val loss: 0.2166 val r: 0.2682; time: 77.37sec
[15/20][22/22] total train loss: 0.0197; total val loss: 0.2142 val r: 0.3207; time: 76.49sec
[16/20][22/22] total train loss: 0.0139; total val loss: 0.2200 val r: 0.3276; time: 80.85sec
[17/20][22/22] total train loss: 0.0126; total val loss: 0.2170 val r: 0.2506; time: 77.60sec
[18/20][22/22] total train loss: 0.0117; total val loss: 0.2166 val r: 0.3041; time: 78.12sec
[19/20][22/22] total train loss: 0.0096; total val loss: 0.2123 val r: 0.2886; time: 77.69sec
learning rate updated: 0.0008
[20/20][22/22] total train loss: 0.0078; total val loss: 0.2257 val r: 0.2951; time: 78.86sec
Best epoch 16 with val_r = 0.3276.
Fold #2
- - - - - - - - - - - - -
initializing neural net
[1/20][22/22] total train loss: 0.7463; total val loss: 0.2116 val r: 0.2764; time: 97.38sec
[2/20][22/22] total train loss: 0.4801; total val loss: 0.2239 val r: 0.1997; time: 80.07sec
[3/20][22/22] total train loss: 0.3998; total val loss: 0.2451 val r: 0.3367; time: 80.98sec
[4/20][22/22] total train loss: 0.2637; total val loss: 0.2373 val r: 0.3776; time: 76.52sec
[5/20][22/22] total train loss: 0.1790; total val loss: 0.2412 val r: 0.3481; time: 80.44sec
[6/20][22/22] total train loss: 0.1349; total val loss: 0.2446 val r: 0.3499; time: 80.57sec
[7/20][22/22] total train loss: 0.0719; total val loss: 0.2488 val r: 0.3435; time: 80.03sec
[8/20][22/22] total train loss: 0.0589; total val loss: 0.2657 val r: 0.3364; time: 80.94sec
[9/20][22/22] total train loss: 0.0387; total val loss: 0.2483 val r: 0.3444; time: 84.65sec
[10/20][22/22] total train loss: 0.0311; total val loss: 0.2863 val r: 0.3474; time: 86.29sec
[11/20][22/22] total train loss: 0.0265; total val loss: 0.2486 val r: 0.3414; time: 82.30sec
[12/20][22/22] total train loss: 0.0219; total val loss: 0.2489 val r: 0.3686; time: 79.42sec
[13/20][22/22] total train loss: 0.0195; total val loss: 0.2642 val r: 0.3550; time: 77.87sec
[14/20][22/22] total train loss: 0.0163; total val loss: 0.2495 val r: 0.3499; time: 82.54sec
[15/20][22/22] total train loss: 0.0157; total val loss: 0.2580 val r: 0.3557; time: 83.74sec
[16/20][22/22] total train loss: 0.0135; total val loss: 0.2465 val r: 0.3347; time: 81.58sec
[17/20][22/22] total train loss: 0.0129; total val loss: 0.2345 val r: 0.3436; time: 78.32sec
[18/20][22/22] total train loss: 0.0129; total val loss: 0.2669 val r: 0.3691; time: 98.75sec
[19/20][22/22] total train loss: 0.0109; total val loss: 0.2560 val r: 0.3472; time: 89.55sec
learning rate updated: 0.0008
[20/20][22/22] total train loss: 0.0090; total val loss: 0.2483 val r: 0.3561; time: 80.38sec
Best epoch 4 with val_r = 0.3776.
Fold #3
- - - - - - - - - - - - -
initializing neural net
[1/20][22/22] total train loss: 0.9020; total val loss: 0.1235 val r: 0.1224; time: 88.36sec
[2/20][22/22] total train loss: 0.5512; total val loss: 0.1349 val r: 0.2104; time: 77.87sec
[3/20][22/22] total train loss: 0.4605; total val loss: 0.1429 val r: 0.2210; time: 76.24sec
[4/20][22/22] total train loss: 0.3172; total val loss: 0.1859 val r: 0.1898; time: 79.90sec
[5/20][22/22] total train loss: 0.1948; total val loss: 0.1835 val r: 0.2209; time: 80.50sec
[6/20][22/22] total train loss: 0.1445; total val loss: 0.2330 val r: 0.2775; time: 82.63sec
[7/20][22/22] total train loss: 0.0983; total val loss: 0.1863 val r: 0.2499; time: 83.87sec
[8/20][22/22] total train loss: 0.0713; total val loss: 0.1761 val r: 0.1913; time: 97.01sec
[9/20][22/22] total train loss: 0.0533; total val loss: 0.1879 val r: 0.2488; time: 82.30sec
[10/20][22/22] total train loss: 0.0390; total val loss: 0.1723 val r: 0.2685; time: 88.25sec
[11/20][22/22] total train loss: 0.0270; total val loss: 0.1853 val r: 0.2913; time: 85.23sec
[12/20][22/22] total train loss: 0.0253; total val loss: 0.1810 val r: 0.2527; time: 81.75sec
[13/20][22/22] total train loss: 0.0242; total val loss: 0.1870 val r: 0.2820; time: 89.20sec
[14/20][22/22] total train loss: 0.0185; total val loss: 0.1811 val r: 0.2915; time: 79.78sec
[15/20][22/22] total train loss: 0.0148; total val loss: 0.1713 val r: 0.3063; time: 79.34sec
[16/20][22/22] total train loss: 0.0157; total val loss: 0.1911 val r: 0.2873; time: 80.01sec
[17/20][22/22] total train loss: 0.0132; total val loss: 0.1820 val r: 0.2879; time: 80.28sec
[18/20][22/22] total train loss: 0.0132; total val loss: 0.1876 val r: 0.2874; time: 78.86sec
[19/20][22/22] total train loss: 0.0102; total val loss: 0.1799 val r: 0.2781; time: 83.06sec
learning rate updated: 0.0008
[20/20][22/22] total train loss: 0.0107; total val loss: 0.1739 val r: 0.3013; time: 79.31sec
Best epoch 15 with val_r = 0.3063.
Fold #4
- - - - - - - - - - - - -
initializing neural net
[1/20][22/22] total train loss: 0.8524; total val loss: 0.1319 val r: 0.1286; time: 87.42sec
[2/20][22/22] total train loss: 0.5386; total val loss: 0.1436 val r: 0.1523; time: 84.35sec
[3/20][22/22] total train loss: 0.4163; total val loss: 0.1537 val r: 0.2541; time: 80.76sec
[4/20][22/22] total train loss: 0.2604; total val loss: 0.2048 val r: 0.2208; time: 82.57sec
[5/20][22/22] total train loss: 0.1797; total val loss: 0.1835 val r: 0.2287; time: 80.29sec
[6/20][22/22] total train loss: 0.1288; total val loss: 0.2240 val r: 0.2397; time: 87.99sec
[7/20][22/22] total train loss: 0.0822; total val loss: 0.1647 val r: 0.2092; time: 82.19sec
[8/20][22/22] total train loss: 0.0659; total val loss: 0.2065 val r: 0.2523; time: 85.39sec
[9/20][22/22] total train loss: 0.0502; total val loss: 0.2055 val r: 0.2272; time: 91.36sec
[10/20][22/22] total train loss: 0.0363; total val loss: 0.1811 val r: 0.2167; time: 95.50sec
[11/20][22/22] total train loss: 0.0295; total val loss: 0.2076 val r: 0.2533; time: 99.81sec
[12/20][22/22] total train loss: 0.0255; total val loss: 0.1731 val r: 0.2479; time: 89.55sec
[13/20][22/22] total train loss: 0.0209; total val loss: 0.1832 val r: 0.2539; time: 93.83sec
[14/20][22/22] total train loss: 0.0155; total val loss: 0.1762 val r: 0.2031; time: 80.68sec
[15/20][22/22] total train loss: 0.0156; total val loss: 0.1949 val r: 0.2407; time: 83.11sec
[16/20][22/22] total train loss: 0.0119; total val loss: 0.1957 val r: 0.2374; time: 82.46sec
[17/20][22/22] total train loss: 0.0127; total val loss: 0.2039 val r: 0.2374; time: 87.79sec
[18/20][22/22] total train loss: 0.0139; total val loss: 0.1892 val r: 0.2576; time: 80.69sec
[19/20][22/22] total train loss: 0.0157; total val loss: 0.1851 val r: 0.2265; time: 74.66sec
learning rate updated: 0.0008
[20/20][22/22] total train loss: 0.0128; total val loss: 0.1891 val r: 0.2473; time: 74.57sec
Best epoch 18 with val_r = 0.2576.
Fold #5
- - - - - - - - - - - - -
initializing neural net
[1/20][22/22] total train loss: 0.9310; total val loss: 0.1584 val r: 0.1016; time: 87.46sec
[2/20][22/22] total train loss: 0.5237; total val loss: 0.1606 val r: 0.3378; time: 88.28sec
[3/20][22/22] total train loss: 0.4306; total val loss: 0.1878 val r: 0.3453; time: 84.24sec
[4/20][22/22] total train loss: 0.2922; total val loss: 0.2252 val r: 0.2973; time: 82.62sec
[5/20][22/22] total train loss: 0.2224; total val loss: 0.2376 val r: 0.2169; time: 81.00sec
[6/20][22/22] total train loss: 0.1199; total val loss: 0.2091 val r: 0.2893; time: 81.19sec
[7/20][22/22] total train loss: 0.0836; total val loss: 0.1965 val r: 0.2628; time: 79.40sec
[8/20][22/22] total train loss: 0.0722; total val loss: 0.2177 val r: 0.2877; time: 79.86sec
[9/20][22/22] total train loss: 0.0548; total val loss: 0.2330 val r: 0.2753; time: 78.63sec
[10/20][22/22] total train loss: 0.0366; total val loss: 0.2138 val r: 0.2651; time: 79.60sec
[11/20][22/22] total train loss: 0.0303; total val loss: 0.2164 val r: 0.2917; time: 81.69sec
[12/20][22/22] total train loss: 0.0234; total val loss: 0.2108 val r: 0.2904; time: 79.40sec
[13/20][22/22] total train loss: 0.0181; total val loss: 0.2025 val r: 0.2734; time: 79.55sec
[14/20][22/22] total train loss: 0.0155; total val loss: 0.2119 val r: 0.2955; time: 78.78sec
[15/20][22/22] total train loss: 0.0135; total val loss: 0.2036 val r: 0.3093; time: 79.32sec
[16/20][22/22] total train loss: 0.0120; total val loss: 0.2074 val r: 0.3009; time: 89.73sec
[17/20][22/22] total train loss: 0.0102; total val loss: 0.2158 val r: 0.2970; time: 92.07sec
[18/20][22/22] total train loss: 0.0115; total val loss: 0.2094 val r: 0.3118; time: 88.99sec
[19/20][22/22] total train loss: 0.0127; total val loss: 0.2042 val r: 0.3029; time: 94.63sec
learning rate updated: 0.0008
[20/20][22/22] total train loss: 0.0113; total val loss: 0.2258 val r: 0.2858; time: 85.37sec
Best epoch 3 with val_r = 0.3453.
Highest avg. r=0.3065 achieved at epoch 15 (on validation set).
Avg. train loss: [0.8575356246903538, 0.5141757057979703, 0.4131855690851808, 0.28110925601795317, 0.1970763267017901, 0.12903042659163474, 0.08364165886305272, 0.06477229120209813, 0.04941632547415793, 0.036526417767163366, 0.029049257922451943, 0.02470417389413342, 0.021704374853288754, 0.01812150051118806, 0.01585914065944962, 0.01339718124945648, 0.012309071753406897, 0.012644333759089932, 0.011837141163414343, 0.010311569896293804]
Avg. validation loss: [0.16091149486601353, 0.17077681105583906, 0.18353287912905217, 0.2187918394804001, 0.21406949684023857, 0.22921505756676197, 0.20366079956293107, 0.2177692960947752, 0.2191695474088192, 0.2154880378395319, 0.21371727101504803, 0.20859620533883572, 0.20680855847895147, 0.20705981999635698, 0.20839162450283766, 0.21212706714868546, 0.2106508355587721, 0.21393030732870102, 0.2074832320213318, 0.21255621761083604]
Avg. validation r: [0.14086084597622878, 0.22276509576953937, 0.27825677288475414, 0.2692829705942812, 0.2649136693813315, 0.2963712681762894, 0.267669468018425, 0.2698223866892978, 0.2833492948260533, 0.27717986331396105, 0.2896907642273092, 0.2953637623977593, 0.2922086797784432, 0.28164555055991697, 0.3065413377939937, 0.29756395868110436, 0.28330532902278344, 0.3060099392757202, 0.28864152325823306, 0.29712042099545616]
Using configurations:
{'BATCH_ITEM_NUM': 30,
 'BERT_LARGE': True,
 'BERT_LAYER': 11,
 'CONFIG_NAME': 'bertlarge_lstm_elissa',
 'CROSS_VALIDATION_FLAG': True,
 'CUDA': False,
 'ELMO_LAYER': 2,
 'ELMO_MODE': 'concat',
 'EVAL': {'BEST_EPOCH': 100, 'FLAG': False},
 'EXPERIMENT_NAME': 'bertlarge_lstm_elissa',
 'GLOVE_DIM': 100,
 'GPU_NUM': 0,
 'IS_BERT': True,
 'IS_ELMO': False,
 'IS_RANDOM': False,
 'KFOLDS': 5,
 'LSTM': {'ATTN': False,
          'BIDIRECTION': True,
          'DROP_PROB': 0.5,
          'FLAG': True,
          'HIDDEN_DIM': 800,
          'LAYERS': 2,
          'SEQ_LEN': 50},
 'MODE': 'train',
 'OUT_PATH': './',
 'PREDICTION_TYPE': 'rating',
 'PREDON': 'train',
 'RESUME_DIR': '',
 'SAVE_PREDS': False,
 'SEED': 0,
 'SINGLE_SENTENCE': True,
 'SOME_DATABASE': './sentence_si_means.csv',
 'SPLIT_NAME': '',
 'TRAIN': {'BATCH_SIZE': 32,
           'COEFF': {'BETA_1': 0.9, 'BETA_2': 0.999, 'EPS': 1e-08},
           'DROPOUT': {'FC_1': 0.75, 'FC_2': 0.75},
           'FLAG': True,
           'INTERVAL': 10,
           'LR': 0.0001,
           'LR_DECAY_EPOCH': 20,
           'LR_DECAY_RATE': 0.8,
           'START_EPOCH': 0,
           'TOTAL_EPOCH': 20}}
Using random seed 0.
Path to the current word embeddings: ./datasets/seed_0/bert_largelayer_11_lstm/embs_train_50.npy
Start training
===============================
Fold #1
- - - - - - - - - - - - -
initializing neural net
[1/20][22/22] total train loss: 0.5578; total val loss: 0.2038 val r: 0.2023; time: 89.46sec
[2/20][22/22] total train loss: 0.4890; total val loss: 0.1767 val r: 0.2676; time: 92.66sec
[3/20][22/22] total train loss: 0.4165; total val loss: 0.1867 val r: 0.3058; time: 75.64sec
[4/20][22/22] total train loss: 0.3640; total val loss: 0.2230 val r: 0.3587; time: 73.59sec
[5/20][22/22] total train loss: 0.3165; total val loss: 0.2079 val r: 0.3413; time: 82.91sec
[6/20][22/22] total train loss: 0.2318; total val loss: 0.2498 val r: 0.3651; time: 80.66sec
[7/20][22/22] total train loss: 0.1841; total val loss: 0.2198 val r: 0.3031; time: 85.90sec
[8/20][22/22] total train loss: 0.1338; total val loss: 0.2401 val r: 0.2877; time: 89.43sec
[9/20][22/22] total train loss: 0.0926; total val loss: 0.2491 val r: 0.3123; time: 82.16sec
[10/20][22/22] total train loss: 0.0848; total val loss: 0.2405 val r: 0.2899; time: 84.64sec
[11/20][22/22] total train loss: 0.0780; total val loss: 0.2351 val r: 0.2823; time: 86.57sec
[12/20][22/22] total train loss: 0.0505; total val loss: 0.2523 val r: 0.3023; time: 77.65sec
[13/20][22/22] total train loss: 0.0506; total val loss: 0.2453 val r: 0.2861; time: 79.68sec
[14/20][22/22] total train loss: 0.0410; total val loss: 0.2533 val r: 0.2527; time: 90.31sec
[15/20][22/22] total train loss: 0.0386; total val loss: 0.2438 val r: 0.2972; time: 99.57sec
[16/20][22/22] total train loss: 0.0274; total val loss: 0.2474 val r: 0.2881; time: 81.37sec
[17/20][22/22] total train loss: 0.0238; total val loss: 0.2436 val r: 0.2455; time: 84.31sec
[18/20][22/22] total train loss: 0.0253; total val loss: 0.2468 val r: 0.2969; time: 82.35sec
Using configurations:
{'BATCH_ITEM_NUM': 30,
 'BERT_LARGE': True,
 'BERT_LAYER': 11,
 'CONFIG_NAME': 'bertlarge_lstm_elissa',
 'CROSS_VALIDATION_FLAG': True,
 'CUDA': False,
 'ELMO_LAYER': 2,
 'ELMO_MODE': 'concat',
 'EVAL': {'BEST_EPOCH': 100, 'FLAG': False},
 'EXPERIMENT_NAME': 'bertlarge_lstm_elissa',
 'GLOVE_DIM': 100,
 'GPU_NUM': 0,
 'IS_BERT': True,
 'IS_ELMO': False,
 'IS_RANDOM': False,
 'KFOLDS': 5,
 'LSTM': {'ATTN': False,
          'BIDIRECTION': True,
          'DROP_PROB': 0.3,
          'FLAG': True,
          'HIDDEN_DIM': 800,
          'LAYERS': 2,
          'SEQ_LEN': 50},
 'MODE': 'train',
 'OUT_PATH': './',
 'PREDICTION_TYPE': 'rating',
 'PREDON': 'train',
 'RESUME_DIR': '',
 'SAVE_PREDS': False,
 'SEED': 0,
 'SINGLE_SENTENCE': True,
 'SOME_DATABASE': './sentence_si_means.csv',
 'SPLIT_NAME': '',
 'TRAIN': {'BATCH_SIZE': 32,
           'COEFF': {'BETA_1': 0.9, 'BETA_2': 0.999, 'EPS': 1e-08},
           'DROPOUT': {'FC_1': 0.75, 'FC_2': 0.75},
           'FLAG': True,
           'INTERVAL': 10,
           'LR': 0.0001,
           'LR_DECAY_EPOCH': 20,
           'LR_DECAY_RATE': 0.8,
           'START_EPOCH': 0,
           'TOTAL_EPOCH': 20}}
Using random seed 0.
Path to the current word embeddings: ./datasets/seed_0/bert_largelayer_11_lstm/embs_train_50.npy
Start training
===============================
Fold #1
- - - - - - - - - - - - -
initializing neural net
[1/20][22/22] total train loss: 0.5588; total val loss: 0.2040 val r: 0.2096; time: 83.38sec
[2/20][22/22] total train loss: 0.4788; total val loss: 0.1787 val r: 0.2742; time: 85.34sec
[3/20][22/22] total train loss: 0.4021; total val loss: 0.1929 val r: 0.2950; time: 81.56sec
[4/20][22/22] total train loss: 0.3437; total val loss: 0.2191 val r: 0.3543; time: 78.00sec
[5/20][22/22] total train loss: 0.2721; total val loss: 0.2143 val r: 0.3400; time: 78.53sec
[6/20][22/22] total train loss: 0.1892; total val loss: 0.2550 val r: 0.3619; time: 76.74sec
[7/20][22/22] total train loss: 0.1441; total val loss: 0.2110 val r: 0.2614; time: 77.20sec
[8/20][22/22] total train loss: 0.1017; total val loss: 0.2394 val r: 0.2577; time: 81.39sec
[9/20][22/22] total train loss: 0.0784; total val loss: 0.2449 val r: 0.3257; time: 96.45sec
[10/20][22/22] total train loss: 0.0666; total val loss: 0.2337 val r: 0.2913; time: 82.51sec
[11/20][22/22] total train loss: 0.0578; total val loss: 0.2323 val r: 0.2805; time: 87.84sec
[12/20][22/22] total train loss: 0.0388; total val loss: 0.2424 val r: 0.3026; time: 81.42sec
[13/20][22/22] total train loss: 0.0343; total val loss: 0.2325 val r: 0.2912; time: 80.59sec
[14/20][22/22] total train loss: 0.0303; total val loss: 0.2421 val r: 0.2585; time: 83.39sec
